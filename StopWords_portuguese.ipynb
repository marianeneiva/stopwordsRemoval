{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "StopWords_portuguese.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMDa48C6mFnsW9+cGM4BU8e",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/marianeneiva/stopwordsRemoval/blob/main/StopWords_portuguese.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7DMOYmj_AFWY"
      },
      "source": [
        "# STOP WORD REMOVAL\n",
        "\n",
        "In text mining one of the strategies to reduce  non relevant data is to remove the 'stop words'. Stop words are commonly used in all documents and texts such as 'is', 'are', 'the' (in English). \n",
        "\n",
        "The removal is often applied in tasks such as classification, search, topic modeling and extraction. The technique also reduces feature vector dimension which is useful for computation cost.\n",
        "\n",
        "#The code\n",
        "\n",
        "In the code below, several motivacional sentences in portuguese are reduced to a set of tokens (words) after the removal of the stop words. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n8DN2rFwk2Nl",
        "outputId": "b3e15394-2f8b-4ea0-e9ae-0cd14bb6a2e1"
      },
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords \n",
        "from nltk.tokenize import word_tokenize \n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "\n",
        "frases = {'Nenhum obstáculo será grande se a sua vontade de vencer for maior',\n",
        "         'Pessoas vencedoras não são aquelas que não falham, são aquelas que não desistem',\n",
        "         'Para ser um campeão você tem que acreditar em si mesmo quando ninguém mais acredita',\n",
        "         'No fim tudo dá certo, e se não deu certo é porque ainda não chegou ao fim',\n",
        "         'Você nunca sabe que resultados virão da sua ação. Mas se você não fizer nada, não existirão resultados',\n",
        "         'Reclamar não é uma estratégia. É necessário lidarmos com o mundo como ele é e não como gostaríamos que ele fosse',\n",
        "         'Inteligência é a capacidade de se adaptar às mudanças'}\n",
        "\n",
        "#we are using stop words in portuguese\n",
        "stop_words = set(stopwords.words('portuguese')) \n",
        "\n",
        "for frase in frases:\n",
        "  word_tokens = word_tokenize(frase) \n",
        "  filtered_sentence = [w for w in word_tokens if not w in stop_words] \n",
        "  print('Tokens with stop words:')\n",
        "  print(word_tokens) \n",
        "  print()\n",
        "  print('Tokens without stop words:')\n",
        "  print(filtered_sentence) \n",
        "  print('\\n\\n')"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "Tokens with stop words:\n",
            "['Nenhum', 'obstáculo', 'será', 'grande', 'se', 'a', 'sua', 'vontade', 'de', 'vencer', 'for', 'maior']\n",
            "\n",
            "Tokens without stop words:\n",
            "['Nenhum', 'obstáculo', 'grande', 'vontade', 'vencer', 'maior']\n",
            "\n",
            "\n",
            "\n",
            "Tokens with stop words:\n",
            "['Inteligência', 'é', 'a', 'capacidade', 'de', 'se', 'adaptar', 'às', 'mudanças']\n",
            "\n",
            "Tokens without stop words:\n",
            "['Inteligência', 'capacidade', 'adaptar', 'mudanças']\n",
            "\n",
            "\n",
            "\n",
            "Tokens with stop words:\n",
            "['No', 'fim', 'tudo', 'dá', 'certo', ',', 'e', 'se', 'não', 'deu', 'certo', 'é', 'porque', 'ainda', 'não', 'chegou', 'ao', 'fim']\n",
            "\n",
            "Tokens without stop words:\n",
            "['No', 'fim', 'tudo', 'dá', 'certo', ',', 'deu', 'certo', 'porque', 'ainda', 'chegou', 'fim']\n",
            "\n",
            "\n",
            "\n",
            "Tokens with stop words:\n",
            "['Pessoas', 'vencedoras', 'não', 'são', 'aquelas', 'que', 'não', 'falham', ',', 'são', 'aquelas', 'que', 'não', 'desistem']\n",
            "\n",
            "Tokens without stop words:\n",
            "['Pessoas', 'vencedoras', 'falham', ',', 'desistem']\n",
            "\n",
            "\n",
            "\n",
            "Tokens with stop words:\n",
            "['Para', 'ser', 'um', 'campeão', 'você', 'tem', 'que', 'acreditar', 'em', 'si', 'mesmo', 'quando', 'ninguém', 'mais', 'acredita']\n",
            "\n",
            "Tokens without stop words:\n",
            "['Para', 'ser', 'campeão', 'acreditar', 'si', 'ninguém', 'acredita']\n",
            "\n",
            "\n",
            "\n",
            "Tokens with stop words:\n",
            "['Você', 'nunca', 'sabe', 'que', 'resultados', 'virão', 'da', 'sua', 'ação', '.', 'Mas', 'se', 'você', 'não', 'fizer', 'nada', ',', 'não', 'existirão', 'resultados']\n",
            "\n",
            "Tokens without stop words:\n",
            "['Você', 'nunca', 'sabe', 'resultados', 'virão', 'ação', '.', 'Mas', 'fizer', 'nada', ',', 'existirão', 'resultados']\n",
            "\n",
            "\n",
            "\n",
            "Tokens with stop words:\n",
            "['Reclamar', 'não', 'é', 'uma', 'estratégia', '.', 'É', 'necessário', 'lidarmos', 'com', 'o', 'mundo', 'como', 'ele', 'é', 'e', 'não', 'como', 'gostaríamos', 'que', 'ele', 'fosse']\n",
            "\n",
            "Tokens without stop words:\n",
            "['Reclamar', 'estratégia', '.', 'É', 'necessário', 'lidarmos', 'mundo', 'gostaríamos']\n",
            "\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}